{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845690ef",
   "metadata": {},
   "source": [
    "# Class Frame (no actual use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f21554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Abstract Base Class for Interchangeability ---\n",
    "\n",
    "class TextEmbedder(ABC):\n",
    "    \"\"\"\n",
    "    Abstract Base Class for text embedders. It ensures that all concrete\n",
    "    embedder classes implement a consistent interface (`fit` and `transform`),\n",
    "    making them easily interchangeable in any ML pipeline.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Learns the vocabulary or model parameters from a collection of documents.\n",
    "        \n",
    "        Args:\n",
    "            documents (list of str): A list of text documents to train on.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Converts a collection of documents into numerical embeddings.\n",
    "\n",
    "        Args:\n",
    "            documents (list of str): A list of text documents to embed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A dense NumPy array of the document embeddings.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"\n",
    "\n",
    "        A convenience method to first fit the model and then transform the documents.\n",
    "        \n",
    "        Args:\n",
    "            documents (list of str): A list of text documents.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A dense NumPy array of the document embeddings.\n",
    "        \"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee53395",
   "metadata": {},
   "source": [
    "# Embedder Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abb73c",
   "metadata": {},
   "source": [
    "## N-grams & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b901857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Finalized Embedder Implementations ---\n",
    "\n",
    "class TfidfEmbedder(TextEmbedder):\n",
    "    \"\"\"\n",
    "    Embeds text by calculating Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    scores for N-grams. This approach emphasizes words that are frequent in a\n",
    "    document but rare across the entire corpus, making it great for keyword\n",
    "    and topic identification.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_features=5000, ngram_range=(1, 2)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_features (int): The maximum number of top N-grams to keep.\n",
    "            ngram_range (tuple): The range of N-grams to consider (e.g., (1, 2)\n",
    "                                 for unigrams and bigrams).\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        print(f\"Initialized TfidfEmbedder with ngram_range={ngram_range}, max_features={max_features}\")\n",
    "\n",
    "    def fit(self, documents):\n",
    "        print(\"Fitting TF-IDF vocabulary...\")\n",
    "        self.vectorizer.fit(documents)\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        print(f\"Transforming {len(documents)} documents into TF-IDF vectors...\")\n",
    "        # Convert the sparse matrix output to a dense numpy array\n",
    "        return self.vectorizer.transform(documents).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33050b93",
   "metadata": {},
   "source": [
    "## Averaged Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186657f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragedWordVectorEmbedder(TextEmbedder):\n",
    "    \"\"\"\n",
    "    Simulates pre-trained word embeddings (like GloVe or Word2Vec). Each document\n",
    "    is represented by the average of its word vectors. This method captures the\n",
    "    semantic \"gist\" of the document but discards word order.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=100, max_features=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): The desired dimensionality of the output vectors.\n",
    "            max_features (int): The vocabulary size to consider.\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer = CountVectorizer(max_features=self.max_features, stop_words='english')\n",
    "        self.embedding_matrix_ = None\n",
    "        print(f\"Initialized AveragedWordVectorEmbedder with embedding_dim={embedding_dim}\")\n",
    "\n",
    "    def fit(self, documents):\n",
    "        print(\"Fitting vocabulary and creating simulated embedding matrix...\")\n",
    "        self.vectorizer.fit(documents)\n",
    "        vocab = self.vectorizer.get_feature_names_out()\n",
    "        # In a real application, you would load a pre-trained matrix.\n",
    "        # Here, we simulate it with random vectors for demonstration.\n",
    "        self.embedding_matrix_ = np.random.randn(len(vocab), self.embedding_dim)\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        if self.embedding_matrix_ is None:\n",
    "            raise RuntimeError(\"Embedder has not been fitted yet.\")\n",
    "        \n",
    "        print(f\"Transforming {len(documents)} documents into averaged word vectors...\")\n",
    "        word_counts = self.vectorizer.transform(documents)\n",
    "        doc_embeddings = np.zeros((word_counts.shape[0], self.embedding_dim))\n",
    "        \n",
    "        for i, doc_vector in enumerate(word_counts):\n",
    "            word_indices = doc_vector.indices\n",
    "            if len(word_indices) > 0:\n",
    "                # Average the vectors of the words present in the document\n",
    "                doc_embeddings[i] = self.embedding_matrix_[word_indices].mean(axis=0)\n",
    "                \n",
    "        return doc_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea7552",
   "metadata": {},
   "source": [
    "## Hashing SVD (dim reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29befcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashingSvdEmbedder(TextEmbedder):\n",
    "    \"\"\"\n",
    "    A memory-efficient and scalable approach that first uses the \"hashing trick\"\n",
    "    to map N-grams to a fixed-size feature space, then applies Truncated SVD\n",
    "    (a form of PCA for sparse data) to create dense, lower-dimensional \"topic\"\n",
    "    vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=100, n_features=2**12, ngram_range=(1, 2)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_components (int): The final embedding dimension after SVD reduction.\n",
    "            n_features (int): The size of the hashing space.\n",
    "            ngram_range (tuple): The range of N-grams to consider.\n",
    "        \"\"\"\n",
    "        self.pipeline = make_pipeline(\n",
    "            HashingVectorizer(n_features=n_features, ngram_range=ngram_range, stop_words='english'),\n",
    "            TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        )\n",
    "        print(f\"Initialized HashingSvdEmbedder with n_components={n_components}\")\n",
    "\n",
    "    def fit(self, documents):\n",
    "        print(\"Fitting the Hashing+SVD pipeline...\")\n",
    "        self.pipeline.fit(documents)\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        print(f\"Transforming {len(documents)} documents with Hashing+SVD...\")\n",
    "        return self.pipeline.transform(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c4385",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Demonstration of Interchangeability ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Sample data for your recommendation system project\n",
    "    game_and_movie_docs = [\n",
    "        \"A tactical first-person shooter with a focus on teamwork.\",\n",
    "        \"Epic fantasy adventure with dragons and mighty magic.\",\n",
    "        \"A sci-fi thriller set in a grim, dystopian future.\",\n",
    "        \"Players explore a vast open world in this fantasy RPG.\",\n",
    "        \"This movie is a thriller about a deep-sea mystery.\",\n",
    "    ]\n",
    "\n",
    "    # A dictionary of embedders to test\n",
    "    embedders_to_test = {\n",
    "        \"TF-IDF\": TfidfEmbedder(max_features=50, ngram_range=(1, 1)),\n",
    "        \"Averaged Word Vectors\": AveragedWordVectorEmbedder(embedding_dim=64),\n",
    "        \"Hashing + SVD\": HashingSvdEmbedder(n_components=32, n_features=1024)\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMONSTRATING INTERCHANGEABLE EMBEDDERS\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    for name, embedder in embedders_to_test.items():\n",
    "        print(f\"--- Processing with: {name} ---\")\n",
    "        \n",
    "        # The fit_transform call is identical for all embedders\n",
    "        embeddings = embedder.fit_transform(game_and_movie_docs)\n",
    "        \n",
    "        print(f\"Output embedding shape: {embeddings.shape}\")\n",
    "        # Verify that the output is a dense NumPy array\n",
    "        assert isinstance(embeddings, np.ndarray)\n",
    "        assert embeddings.ndim == 2\n",
    "        assert embeddings.shape[0] == len(game_and_movie_docs)\n",
    "        print(\"Successfully produced a dense NumPy array.\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
